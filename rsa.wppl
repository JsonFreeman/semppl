/// <reference path="webppl.d.ts" />
/// <reference types="underscore" />
// webppl rsa.wppl --require .
var cartesianProductMap = function (fn, arr1, arr2) {
    return _.flatten(map(function (e1) {
        return map(function (e2) {
            return fn(e1, e2);
        }, arr2);
    }, arr1));
}

var featureFn = function (d) { return semppl.ruleFeatureFn(d); };
var scoreFn = function (features, params) {
    return sum(_.values(mapObject(function (k, v) {
        return _.has(params, k) ? v * params[k] : 0;
    }, features)));
};

var normalize = function (array) {
    var s = sum(array);
    return map(function (x) {
        return x / s;
    }, array);
};

var denotationProbability = function (world) {
    return function (derivation) {
        return derivation.semantics(world);
    };
};

var createParser = function (grammar, params) {
    var parser = semppl.createParser(grammar, params, semppl.ruleFeatureFn);
    return cache(function (utterance, theta, startSymbol) {
        // Need to use .call to call parser as a js function
        var chart = parser.call(null, utterance, theta);
        var rootCellDerivations = semppl.getRootCellDerivations(chart, startSymbol);

        // Gather the two relevant probabilities
        var derivationProbabilities = normalize(map(function (d) {
            return Math.exp(scoreFn(featureFn(d), params.parserWeights));
        }, rootCellDerivations));

        return cache(function (world) {
            // World dependent part starts here.
            var truthProbabilities = map(denotationProbability(world), rootCellDerivations);

            // We now have probabilities that the sentences are true given derivations P(t | d)
            // Also probabilities that derivations are correct P(d)

            // Marginalize out derivation by summing over derivations, multiply the two probabilities
            return sum(map2(function (derivation, truth) {
                return derivation * truth;
            }, derivationProbabilities, truthProbabilities));
        });
    });
}

var createParserWeights = function (grammar) {
    return _.object(map(function (entry) {
        var name = semppl.makeRuleKey(entry);
        return [
            name, // key
            modelParam({ mu: 0, name: name }) // value
        ];
    }, grammar));
};

var getGrammarValues = function(params) {
    return mapObject(function(k, v) {
        return v[0].data[0];
    }, params)
}

var literalListenerFn = function (grammar, params) {
    var parse = createParser(grammar, params);
    return cache(function (utterance, theta, worlds) {
        Infer({ method: 'enumerate' }, function () {
            var truthFn = parse(utterance, theta, "$S");
            var world = uniformDraw(worlds);
            factor(Math.log(truthFn(world)));
            return world;
        })
    })
}

var speakerFn = function (grammar, params) {
    var literalListener = literalListenerFn(grammar, params);
    return cache(function (world, theta, worlds, utterances) {
        Infer({ method: 'enumerate' }, function () {
            var utterance = uniformDraw(utterances);
            var L = literalListener(utterance, theta, worlds);
            factor(L.score(world));
            return utterance;
        })
    })
}

var listenerFn = function (grammar, params) {
    var speaker = speakerFn(grammar, params);
    return cache(function (utterance, thetaCandidates, worlds, utterances) {
        Infer({ method: 'enumerate' }, function () {
            var theta = uniformDraw(thetaCandidates);
            var world = uniformDraw(worlds);
            var S = speaker(world, theta, worlds, utterances);
            factor(S.score(utterance));
            return world;
        })
    })
}

var createNetworkParams = function (hiddenSize, inputSize) {
    var W0 = modelParam({ dims: [hiddenSize, inputSize], mu: 0, sigma: 0.1 })
    var W1 = modelParam({ dims: [1, hiddenSize], mu: 0, sigma: 0.1 })
    var b0 = modelParam({ dims: [hiddenSize, 1], mu: 0, sigma: 0.1 })
    var b1 = modelParam({ dims: [1, 1], mu: 0, sigma: 0.1 })

    return { W: [W0, W1], b: [b0, b1] };
}

var worldsFn = function (h, w) {
    return {
        facts: {
            height: { john: h, gates: h },
            weight: { john: w, gates: w },
            man: { john: true, gates: false },
            building: { john: false, gates: true },
        },
        domain: ["john", "gates"]
    };
};

var createModel = function (grammar, dataTriples) {
    return function () {
        var scalarPredicates = ['tall', 'heavy', 'man', 'building'];

        var params = {
            parserWeights: createParserWeights(grammar),
            networkParams: _.object(scalarPredicates, repeat(scalarPredicates.length, function () {
                createNetworkParams(10, scalarPredicates.length)
            }))
        };

        var listener = listenerFn(grammar, params);

        if (dataTriples) {
            mapData({ data: dataTriples }, function (example) {
                var world = example[0];
                var utterance = example[1];
                var context = example[2];
                var distribution = listener(utterance, context.thetaCandidates, context.worlds, context.utterances);
                observe(distribution, world);
            });
        }

        return listener;
    };
};

var sampleGroundDistributions = function (utterances, contexts, numSamplesEach) {
    var listener = listenerFn(semppl.fixedGrammar, {});
    return _.flatten(map(function (u) {
        return _.flatten(map(function (c) {
            return repeat(numSamplesEach, function () {
                return [sample(listener(u, c.thetaCandidates, c.worlds, c.utterances)), u, c];
            });
        }, contexts), /*shallow*/ true);
    }, utterances), /*shallow*/ true);
}

var train = function (dataPairs, iterations, formerTrainingResults) {
    var trainingModel =
        createModel(semppl.ambiguousGrammar, dataPairs);
    return Optimize(trainingModel, {
        steps: iterations,
        estimator: { ELBO: { samples: 1 } },
        params: formerTrainingResults
    });
};

var test = function (utterance, context, trainingResults) {
    // Ensure the utterances are in the context
    var newContext = _.create(context, {
        utterances: _.uniq(context.utterances.concat([utterance]))
    })
    var testModel = createModel(semppl.ambiguousGrammar, null);
    var listener = sample(SampleGuide(testModel, { params: trainingResults }));
    display("utterance: " + utterance)
    display(listener(utterance, newContext.thetaCandidates, newContext.worlds, newContext.utterances))
}


var trainOnPeopleTestOnBuildings = function() {
    // Training setup
    var peopleHeights = [4, 6, 8];
    var peopleWeights = [100, 150, 200];
    var buildingHeights = [50, 100, 150];
    var buildingWeights = [500, 600, 700]

    var peopleWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { john: uniformDraw(peopleHeights) },
                weight: { john: uniformDraw(peopleWeights) },
                person: { john: true },
                building: { john: false }
            }
        }
    }).support();

    var thetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, peopleHeights, peopleWeights);

    var utterances = ["John is tall", "John is heavy", "John is light", "John is short"];
    var context = {
        worlds: peopleWorlds,
        utterances: utterances,
        thetaCandidates: thetas
    }
    var data = sampleGroundDistributions(utterances, [context], 200);
    var results = train(data, 2000, null);

    test("John is tall", context, results);
    test("John is heavy", context, results);
    // Make buildings worlds
    var buildingWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { gates: uniformDraw(buildingHeights) },
                weight: { gates: uniformDraw(buildingWeights) },
                person: { gates: false },
                building: { gates: true }
            }
        }
    }).support();
    var buildingThetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, buildingHeights, buildingWeights);
    var testUtterances = ["Gates is tall", "Gates is heavy", "Gates is light", "Gates is short"];
    test("Gates is tall", {
        worlds: buildingWorlds,
        utterances: testUtterances,
        thetaCandidates: thetas
    }, results);
    test("Gates is heavy", {
        worlds: buildingWorlds,
        utterances: testUtterances,
        thetaCandidates: thetas
    }, results);
}
// trainOnPeopleTestOnBuildings();

var trainOnPeopleTestOnBuildingsJustScale = function() {
    // Training setup
    var peopleHeights = [4, 6, 8];
    var peopleWeights = [100, 150, 200];
    var buildingHeights = [40, 60, 80];
    var buildingWeights = [300, 450, 600]

    var peopleWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { john: uniformDraw(peopleHeights) },
                weight: { john: uniformDraw(peopleWeights) },
                person: { john: true },
                building: { john: false }
            }
        }
    }).support();

    var thetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, peopleHeights, peopleWeights);

    var utterances = ["John is tall", "John is heavy", "John is light", "John is short"];
    var context = {
        worlds: peopleWorlds,
        utterances: utterances,
        thetaCandidates: thetas
    }
    var data = sampleGroundDistributions(utterances, [context], 200);
    var results = train(data, 2000, null);

    test("John is tall", context, results);
    test("John is heavy", context, results);
    // Make buildings worlds
    var buildingWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { gates: uniformDraw(buildingHeights) },
                weight: { gates: uniformDraw(buildingWeights) },
                person: { gates: false },
                building: { gates: true }
            }
        }
    }).support();
    var buildingThetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, buildingHeights, buildingWeights);
    var testUtterances = ["Gates is tall", "Gates is heavy", "Gates is light", "Gates is short"];
    test("Gates is tall", {
        worlds: buildingWorlds,
        utterances: testUtterances,
        thetaCandidates: thetas
    }, results);
    test("Gates is heavy", {
        worlds: buildingWorlds,
        utterances: testUtterances,
        thetaCandidates: thetas
    }, results);
}
// trainOnPeopleTestOnBuildingsJustScale();

var trainOnPeopleTreesTestOnBuildingsJustScale = function() {
    // Training setup
    var peopleHeights = [4, 6, 8];
    var peopleWeights = [100, 150, 200];
    var treeHeights = [10, 15, 20];
    var treeWeights = [200, 300, 400];
    var buildingHeights = [20, 30, 40];
    var buildingWeights = [300, 450, 600]

    var peopleWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { john: uniformDraw(peopleHeights) },
                weight: { john: uniformDraw(peopleWeights) },
                person: { john: true  },
                building: { john: false }
            }
        }
    }).support();

    var treeWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { oak: uniformDraw(treeHeights) },
                weight: { oak: uniformDraw(treeWeights) },
                person: { oak: false  },
                building: { oak: false }
            }
        }
    }).support();

    var peopleThetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, peopleHeights, peopleWeights);

    var treeThetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, treeHeights, treeWeights);

    var peopleUtterances = ["John is tall", "John is heavy", "John is light", "John is short"];
    var treeUtterances = ["Oak is tall", "Oak is heavy", "Oak is light", "Oak is short"];

    var peopleContext = {
        worlds: peopleWorlds,
        utterances: peopleUtterances,
        thetaCandidates: peopleThetas
    }
    var treeContext = {
        worlds: treeWorlds,
        utterances: treeUtterances,
        thetaCandidates: treeThetas
    }
    var peopleData = sampleGroundDistributions(peopleUtterances, [peopleContext], 100);
    var treeData = sampleGroundDistributions(treeUtterances, [treeContext], 100);
    var results = train(peopleData.concat(treeData), 2000, null);

    // Make buildings worlds
    var buildingWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { gates: uniformDraw(buildingHeights) },
                weight: { gates: uniformDraw(buildingWeights) },
                person: { gates: false },
                building: { gates: true }
            }
        }
    }).support();
    var buildingThetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, buildingHeights, buildingWeights);
    var testUtterances = ["Gates is tall", "Gates is heavy", "Gates is light", "Gates is short"];
    test("Gates is tall", {
        worlds: buildingWorlds,
        utterances: testUtterances,
        thetaCandidates: buildingThetas
    }, results);
    test("Gates is heavy", {
        worlds: buildingWorlds,
        utterances: testUtterances,
        thetaCandidates: buildingThetas
    }, results);
}
trainOnPeopleTreesTestOnBuildingsJustScale();

var trainOnBothTestOnPeople = function() {
    // Training setup
    var peopleHeights = [5, 7];
    var peopleWeights = [100, 200];
    var buildingHeights = [50, 150];
    var buildingWeights = [500, 700];
    
    var trainingWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { john: uniformDraw(peopleHeights), gates: uniformDraw(buildingHeights) },
                weight: { john: uniformDraw(peopleWeights), gates: uniformDraw(buildingWeights) },
                person: { john: true, gates: false },
                building: { john: false, gates: true }
            }
        }
    }).support();

    var thetas = map2(function(h, w) {
        return {
            tall: h,
            heavy: w
        };
    }, peopleHeights.concat(buildingHeights), peopleWeights.concat(buildingWeights));

    var utterances = ["John is tall", "John is heavy", "John is light", "John is short",
                      "Gates is tall", "Gates is heavy", "Gates is light", "Gates is short"];
    var context = {
        worlds: trainingWorlds,
        utterances: utterances,
        thetaCandidates: thetas
    }
    var data = sampleGroundDistributions(utterances, [context], 200);
    var results = train(data, 1000, null);

    // Make buildings worlds
    var peopleWorlds = Infer({method: 'enumerate'}, function() {
        return {
            facts: {
                height: { john: uniformDraw(peopleHeights) },
                weight: { john: uniformDraw(peopleWeights) },
                person: { john: true },
                building: { john: false }
            }
        }
    }).support();

    var testUtterances = ["John is tall", "John is heavy", "John is light", "John is short"];
    test("John is tall", {
        worlds: peopleWorlds,
        utterances: testUtterances,
        thetaCandidates: thetas
    }, results);
    test("John is heavy", {
        worlds: peopleWorlds,
        utterances: testUtterances,
        thetaCandidates: thetas
    }, results);
}
// trainOnBothTestOnPeople();

var testAnd = function() {
    var utterances = ["John is tall", "John is heavy", "John is light", "John is short", "John is heavy and John is short"];
    var peopleWorlds = cartesianProductMap(worldsFn, [8, 6, 4], [8, 6, 4]);
    var thetaValues = [8, 6, 4];
    var context = {
        worlds: peopleWorlds,
        utterances: utterances,
        thetaCandidates: map(function (theta) {
            return {
                tall: theta,
                heavy: theta
            };
        }, thetaValues)
    };
    
    var data = sampleGroundDistributions(utterances, [context], 200);
    var result = train(data, 2000, null);
    test("John is tall and John is heavy", context, result);
}
// testAnd();

var testOr = function() {
    var utterances = ["John is tall", "John is heavy", "John is light", "John is short", "John is heavy or John is short"];
    var peopleWorlds = cartesianProductMap(worldsFn, [8, 6, 4], [8, 6, 4]);
    var thetaValues = [8, 6, 4];
    var context = {
        worlds: peopleWorlds,
        utterances: utterances,
        thetaCandidates: map(function (theta) {
            return {
                tall: theta,
                heavy: theta
            };
        }, thetaValues)
    };
    
    var data = sampleGroundDistributions(utterances, [context], 200);
    var result = train(data, 2000, null);
    test("John is tall or John is heavy", context, result);
}
// testOr();

var testNot = function() {
    var utterances = ["John is tall", "John is heavy", "John is light", "John is short", "John is not heavy"];
    var peopleWorlds = cartesianProductMap(worldsFn, [8, 6, 4], [8, 6, 4]);
    var thetaValues = [8, 6, 4];
    var context = {
        worlds: peopleWorlds,
        utterances: utterances,
        thetaCandidates: map(function (theta) {
            return {
                tall: theta,
                heavy: theta
            };
        }, thetaValues)
    };
    
    var data = sampleGroundDistributions(utterances, [context], 200);
    var result = train(data, 2000, null);
    test("John is not tall", context, result);
}
// testNot();