/// <reference path="webppl.d.ts" />
/// <reference types="underscore" />
// webppl rsa.wppl --require .
var cartesianProductMap = function (fn, arr1, arr2) {
    return _.flatten(map(function (e1) {
        return map(function (e2) {
            return fn(e1, e2);
        }, arr2);
    }, arr1));
}

var featureFn = function (d) { return semppl.ruleFeatureFn(d); };
var scoreFn = function (features, params) {
    return sum(_.values(mapObject(function (k, v) {
        return _.has(params, k) ? v * params[k] : 0;
    }, features)));
};

var normalize = function (array) {
    var s = sum(array);
    return map(function (x) {
        return x / s;
    }, array);
};

var denotationProbability = function (world) {
    return function (derivation) {
        return derivation.semantics(world);
    };
};

var createParser = function (grammar, params) {
    var parser = semppl.createParser(grammar, params, semppl.ruleFeatureFn);
    return cache(function (utterance, theta, startSymbol) {
        // Need to use .call to call parser as a js function
        var chart = parser.call(null, utterance, theta);
        var rootCellDerivations = semppl.getRootCellDerivations(chart, startSymbol);

        // Gather the two relevant probabilities
        var derivationProbabilities = normalize(map(function (d) {
            return Math.exp(scoreFn(featureFn(d), params.parserWeights));
        }, rootCellDerivations));

        return cache(function (world) {
            // World dependent part starts here.
            var truthProbabilities = map(denotationProbability(world), rootCellDerivations);

            // We now have probabilities that the sentences are true given derivations P(t | d)
            // Also probabilities that derivations are correct P(d)

            // Marginalize out derivation by summing over derivations, multiply the two probabilities
            return sum(map2(function (derivation, truth) {
                return derivation * truth;
            }, derivationProbabilities, truthProbabilities));
        });
    });
}

var createParserWeights = function (grammar) {
    return _.object(map(function (entry) {
        var name = semppl.makeRuleKey(entry);
        return [
            name, // key
            modelParam({ mu: 0, name: name }) // value
        ];
    }, grammar));
};

var literalListenerFn = function (grammar, params) {
    var parse = createParser(grammar, params);
    return cache(function (utterance, theta, worlds) {
        Infer({ method: 'enumerate' }, function () {
            var truthFn = parse(utterance, theta, "$S");
            var world = uniformDraw(worlds);
            factor(Math.log(truthFn(world)));
            return world;
        })
    })
}

var speakerFn = function (grammar, params) {
    var literalListener = literalListenerFn(grammar, params);
    return cache(function (world, theta, worlds, utterances) {
        Infer({ method: 'enumerate' }, function () {
            var utterance = uniformDraw(utterances);
            var L = literalListener(utterance, theta, worlds);
            factor(L.score(world));
            return utterance;
        })
    })
}

var listenerFn = function (grammar, params) {
    var speaker = speakerFn(grammar, params);
    return cache(function (utterance, thetaCandidates, worlds, utterances) {
        Infer({ method: 'enumerate' }, function () {
            var theta = uniformDraw(thetaCandidates);
            var world = uniformDraw(worlds);
            var S = speaker(world, theta, worlds, utterances);
            factor(S.score(utterance));
            return world;
        })
    })
}

var createNetworkParams = function (hiddenSize, inputSize) {
    var W0 = modelParam({ dims: [hiddenSize, inputSize], mu: 0, sigma: 0.1 })
    var W1 = modelParam({ dims: [1, hiddenSize], mu: 0, sigma: 0.1 })
    var b0 = modelParam({ dims: [hiddenSize, 1], mu: 0, sigma: 0.1 })
    var b1 = modelParam({ dims: [1, 1], mu: 0, sigma: 0.1 })

    return { W: [W0, W1], b: [b0, b1] };
}

var worldsFn = function (h, w) {
    return {
        facts: {
            height: { john: h, gates: h },
            weight: { john: w, gates: w },
            man: { john: true, gates: false },
            building: { john: false, gates: true },
        },
        domain: ["john", "gates"]
    };
};
var peopleWorlds = cartesianProductMap(worldsFn, [8, 6, 4], [8, 6, 4]);
var buildingsWorlds = cartesianProductMap(worldsFn, [80, 60, 40], [80, 60, 40]);
var thetaValues = [8, 6, 4];
var utterances = ["John is tall", "John is heavy", "John is a man", "John is light", "John is short", "John is a heavy man"];
var context = {
    worlds: peopleWorlds,
    utterances: utterances,
    thetaCandidates: map(function (theta) {
        return {
            tall: theta,
            heavy: theta
        };
    }, thetaValues)
}

var createModel = function (grammar, dataTriples) {
    return function () {
        var scalarPredicates = ['tall', 'heavy', 'man', 'building'];

        var params = {
            parserWeights: createParserWeights(grammar),
            networkParams: _.object(scalarPredicates, repeat(scalarPredicates.length, function () {
                createNetworkParams(10, scalarPredicates.length)
            }))
        };

        var listener = listenerFn(grammar, params);

        if (dataTriples) {
            mapData({ data: dataTriples }, function (example) {
                var world = example[0];
                var utterance = example[1];
                var context = example[2];
                var distribution = listener(utterance, context.thetaCandidates, context.worlds, context.utterances);
                observe(distribution, world);
            });
        }

        return listener;
    };
};

var sampleGroundDistributions = function (utterances, contexts, numSamplesEach) {
    var listener = listenerFn(semppl.fixedGrammar, {});
    return _.flatten(map(function (u) {
        return _.flatten(map(function (c) {
            return repeat(numSamplesEach, function () {
                return [sample(listener(u, c.thetaCandidates, c.worlds, c.utterances)), u, c];
            });
        }, contexts), /*shallow*/ true);
    }, utterances), /*shallow*/ true);
}

var train = function (dataPairs, iterations, formerTrainingResults) {
    var trainingModel =
        createModel(semppl.ambiguousGrammar, dataPairs);
    return Optimize(trainingModel, {
        steps: iterations,
        estimator: { ELBO: { samples: 1 } },
        params: formerTrainingResults
    });
};

var test = function (utterance, context, trainingResults) {
    // Ensure the utterances are in the context
    var newContext = _.create(context, {
        utterances: _.uniq(context.utterances.concat(utterances))
    })
    var testModel = createModel(semppl.ambiguousGrammar, null);
    var listener = sample(SampleGuide(testModel, { params: trainingResults }));
    display("utterance: " + utterance)
    display(listener(utterance, newContext.thetaCandidates, newContext.worlds, newContext.utterances))
}

var data = sampleGroundDistributions(utterances, [context], 200);
var result = train(data, 2000, null);
test("John is a tall man", context, result);
/**
 * mapData
 * cache issue - close over params
 * Tall versus heavy (or even big)
 * Try to do some compositional learning
 * Sample the class explicitly
 * "is a tall man" + "is a tall building"
 * 
 * Tasks: 
 *  - Generate data using ground truth
 *  +!- Try to do caching refactor
 *  +!- Introduce tall, heavy, big with a neural net for each
 *  - Refactor features and weights to add distinguisher
 *  - Implement intersective adjectives
 *  - Compositional learning:
 *    - Intersecting predicates: (multiply, first, second)
 *    - Different ways of applying predicate to entity: (apply to input, ignore input)
 *    - Introduce "John is not a tall man" - not(tall * man), not(tall) * man, not(man) * tall, not(man)
 *    - Try ambiguities with conjunction + disjunction
 */