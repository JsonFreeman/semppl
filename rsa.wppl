/// <reference path="webppl.d.ts" />
/// <reference types="underscore" />
// webppl rsa.wppl --require .
var cartesianProductMap = function (fn, arr1, arr2) {
    return _.flatten(map(function (e1) {
        return map(function (e2) {
            return fn(e1, e2);
        }, arr2);
    }, arr1));
}

var featureFn = function (d) { return semppl.ruleFeatureFn(d); };
var scoreFn = function (features, params) {
    return sum(_.values(mapObject(function (k, v) {
        return _.has(params, k) ? v * params[k] : 0;
    }, features)));
};

var normalize = function (array) {
    var s = sum(array);
    return map(function (x) {
        return x / s;
    }, array);
};

var getSemantics = function (world) {
    return function (derivation) {
        return derivation.semantics(world);
    };
};

var marginalizeDerivationsForAssertion = function(derivationProbabilities, semantics) {
    return sum(map2(function (derivation, truth) {
        return derivation * truth;
    }, derivationProbabilities, semantics));
}

var marginalizeDerivationsForQuestion = function(derivationProbabilities, semantics) {
    return function(element) {
        return sum(map2(function (derivation, truthFn) {
            // truthFn was returned by the parser, so need to call it as a js function
            return derivation * truthFn.call(null, element);
        }, derivationProbabilities, semantics));
    }
}

var createParser = function (grammar, params) {
    var parser = semppl.createParser(grammar, params, semppl.ruleFeatureFn);
    return cache(function (utterance, theta, startSymbol) {
        // Need to use .call to call parser as a js function
        var chart = parser.call(null, utterance, theta);
        var rootCellDerivations = semppl.getRootCellDerivations(chart, startSymbol);

        assert.ok(chart && rootCellDerivations && rootCellDerivations.length > 0,
            "Cannot parse utterance '" + utterance + "' with startSymbol " + startSymbol)
        // Gather the two relevant probabilities
        var derivationProbabilities = normalize(map(function (d) {
            return Math.exp(scoreFn(featureFn(d), params.parserWeights));
        }, rootCellDerivations));

        var marginalizationFunction = startSymbol === "$S" ? marginalizeDerivationsForAssertion :
                                      startSymbol === "$WH" ? marginalizeDerivationsForQuestion :
                                      null;

        return cache(function (world) {
            // World dependent part starts here.
            var semantics = map(getSemantics(world), rootCellDerivations);

            // We now have probabilities that the sentences are true given derivations P(t | d)
            // Also probabilities that derivations are correct P(d)

            return marginalizationFunction(derivationProbabilities, semantics);
        });
    });
}

var createParserWeights = function (grammar) {
    return _.object(map(function (entry) {
        var name = semppl.makeRuleKey(entry);
        return [
            name, // key
            modelParam({ mu: 0, name: name }) // value
        ];
    }, grammar));
};

var getGrammarValues = function(params) {
    return mapObject(function(k, v) {
        return v[0].data[0];
    }, params)
}

var smoothingConstant = 1e-300;
var literalListenerFn = function (grammar, params) {
    var parse = createParser(grammar, params);
    return cache(function (utterance, theta, worlds) {
        Infer({ method: 'enumerate' }, function () {
            var truthFn = parse(utterance, theta, "$S");
            var world = uniformDraw(worlds);
            var smoothedTruthValue = truthFn(world) + smoothingConstant;
            factor(Math.log(smoothedTruthValue));
            return world;
        })
    })
}

var speakerFn = function (grammar, params) {
    var literalListener = literalListenerFn(grammar, params);
    return cache(function (world, theta, worlds, utterances) {
        Infer({ method: 'enumerate' }, function () {
            var utterance = uniformDraw(utterances);
            var L = literalListener(utterance, theta, worlds);
            factor(L.score(world));
            return utterance;
        })
    })
}

var listenerFn = function (grammar, params) {
    var speaker = speakerFn(grammar, params);
    return cache(function (utterance, thetaCandidates, worlds, utterances) {
        Infer({ method: 'enumerate' }, function () {
            var theta = uniformDraw(thetaCandidates);
            var world = uniformDraw(worlds);
            var S = speaker(world, theta, worlds, utterances);
            factor(S.score(utterance));
            return world;
        })
    })
}

var createNetworkParams = function (hiddenSize, inputSize) {
    var W0 = modelParam({ dims: [hiddenSize, inputSize], mu: 0, sigma: 0.1 })
    var W1 = modelParam({ dims: [1, hiddenSize], mu: 0, sigma: 0.1 })
    var b0 = modelParam({ dims: [hiddenSize, 1], mu: 0, sigma: 0.1 })
    var b1 = modelParam({ dims: [1, 1], mu: 0, sigma: 0.1 })

    return { W: [W0, W1], b: [b0, b1] };
}

var worldsFn = function (h, w) {
    return {
        facts: {
            height: { john: h, gates: h },
            weight: { john: w, gates: w },
            man: { john: true, gates: false },
            building: { john: false, gates: true },
        },
        domain: ["john", "gates"]
    };
};

var createModel = function (grammar, dataTriples) {
    return function () {
        var scalarPredicates = ['tall', 'heavy', 'man', 'building'];

        var params = {
            parserWeights: createParserWeights(grammar),
            networkParams: _.object(scalarPredicates, repeat(scalarPredicates.length, function () {
                createNetworkParams(10, scalarPredicates.length)
            }))
        };

        var listener = listenerFn(grammar, params);

        if (dataTriples) {
            mapData({ data: dataTriples, batchSize: dataTriples.length / 5 }, function (example) {
                var world = example[0];
                var utterance = example[1];
                var context = example[2];
                var distribution = listener(utterance, context.thetaCandidates, context.worlds, context.utterances);
                observe(distribution, world);
            });
        }

        return listener;
    };
};

var sampleGroundDistributions = function (utterances, contexts, numSamplesEach) {
    var listener = listenerFn(semppl.fixedGrammar, {});
    return _.flatten(map(function (u) {
        return _.flatten(map(function (c) {
            return repeat(numSamplesEach, function () {
                return [sample(listener(u, c.thetaCandidates, c.worlds, c.utterances)), u, c];
            });
        }, contexts), /*shallow*/ true);
    }, utterances), /*shallow*/ true);
}

var train = function (dataPairs, iterations, formerTrainingResults) {
    var trainingModel =
        createModel(semppl.ambiguousGrammar, dataPairs);
    return Optimize(trainingModel, {
        steps: iterations,
        estimator: { ELBO: { samples: 1 } },
        params: formerTrainingResults
    });
};

var test = function (utterance, context, trainingResults) {
    // Ensure the utterances are in the context
    var newContext = _.create(context, {
        utterances: _.uniq(context.utterances.concat([utterance]))
    })
    display("utterance: " + utterance)
    // display("Ground truth")
    // var groundTruthListener = listenerFn(semppl.fixedGrammar, {});
    // display(groundTruthListener(utterance, newContext.thetaCandidates, newContext.worlds, newContext.utterances))
    var testModel = createModel(semppl.ambiguousGrammar, null);
    var listener = sample(SampleGuide(testModel, { params: trainingResults }));
    // display("Result")
    display(listener(utterance, newContext.thetaCandidates, newContext.worlds, newContext.utterances))
}