// webppl rsa.wppl --require .
var nnparam = function(a, b) {
  return tensorParam([a, b], 0, 0.1);
};

var featureFn = function(d) { return semppl.ruleFeatureFn(d); };
var scoreFn = function(features, params) {
    return sum(_.values(mapObject(function(k, v) {
        return _.has(params, k) ? v * params[k] : 0;
    }, features)));
};

var normalize = function(array) {
    var s = sum(array);
    return map(function(x) {
        return x / s;
    }, array);
};

var denotationProbability = function(world) {
    return function(derivation) {
        return derivation.semantics(world);
    };
};

var parser = semppl.createParser(semppl.grammarIsTall, semppl.ruleFeatureFn);

var parse = function(utterance, params, startSymbol) {
    // Need to use .call to call parser as a js function
    var chart = parser.call(null, utterance, params);
    var rootCellDerivations = semppl.getRootCellDerivations(chart, startSymbol);

    // Gather the two relevant probabilities
    var derivationProbabilities = normalize(map(function(d) {
        return Math.exp(scoreFn(featureFn(d), params.parserWeights));
    }, rootCellDerivations));

    return function(world) {
        // World dependent part starts here.
        var truthProbabilities = map(denotationProbability(world), rootCellDerivations);
        
        // We now have probabilities that the sentences are true given derivations P(t | d)
        // Also probabilities that derivations are correct P(d)

        // Marginalize out derivation by summing over derivations, multiply the two probabilities
        return sum(map2(function(derivation, truth) {
            return derivation * truth;
        }, derivationProbabilities, truthProbabilities));
    };
};

var sampleUniformPrior = function(support) {
    return support[randomInteger(support.length)];
};

var addTheta = function(params, theta) {
    return _.assign(_.clone(params), { theta: theta });
}

var literalListener = function(utterance, params, worlds) {
    Infer({method: 'enumerate'}, function() {
        var truthFn = parse(utterance, params, "$S");
        var world = sampleUniformPrior(worlds);
        factor(Math.log(truthFn(world)));
        return world;
    })
}

var speaker = function(world, params, worlds, utterances) {
    Infer({method: 'enumerate'}, function() {
        var utterance = sampleUniformPrior(utterances);
        var L = literalListener(utterance, params, worlds);
        factor(L.score(world));
        return utterance;
    })
}

var listener = function(utterance, params, thetaCandidates, worlds, utterances) {
    Infer({method: 'enumerate'}, function() {
        var theta = sampleUniformPrior(thetaCandidates);
        var world = sampleUniformPrior(worlds);
        var S = speaker(world, addTheta(params, theta), worlds, utterances);
        factor(S.score(utterance));
        return world;
    })
}

var sampleMatrixGaussian = function(dims,mean,variance,guide_mean){
  var length = dims[0]*dims[1]
  var g = sample(DiagCovGaussian({ mu: Vector(repeat(length, constF(mean))), sigma: Vector(repeat(length, constF(variance)))}),
    {guide: DiagCovGaussian({ mu: T.reshape(guide_mean,[length,1]), sigma: Vector(repeat(length, constF(0.001)))})})
  return T.reshape(g,dims)
}

var sampleScalarGaussian = function(mean, variance) {
    return sample(Gaussian({ mu: mean, sigma: variance }),
        { guide: Gaussian({ mu: scalarParam(mean), sigma: 0.001 }) });
}

var factorConjunction = function(dist, support) {
    var probabilityOfConjunction = sum(map(function(e) {
        return dist.score(e);
    }, support));
    factor(probabilityOfConjunction);
}

var createParserWeights = function(grammar) {
    var arrow = ' -> ';
    return _.object(map(function(entry) {
        return [
            entry.LHS + arrow + entry.RHS, // key
            sampleScalarGaussian(0, 1) // value
        ];
    }, grammar));
};

var model = function() {
    var h_size = 10
    var ent_size = 3

    var W0_var = nnparam(h_size,ent_size)
    var W1_var = nnparam(1,h_size)
    var b0_var = nnparam(h_size,1)
    var b1_var = nnparam(1,1)

    var W0 = sampleMatrixGaussian([h_size,ent_size],0,10,W0_var)
    var W1 = sampleMatrixGaussian([1,h_size],0,10,W1_var)
    var b0 = sampleMatrixGaussian([h_size,1],0,10,b0_var)
    var b1 = sampleMatrixGaussian([1,1],0,10,b1_var)

    var worldsFromHeightsFn = function(h) {
        return {
            facts: { height: { john: h } },
            domain: ["john"]
        };
    };
    var peopleWorlds = map(worldsFromHeightsFn, [8, 6, 4]);
    var buildingsWorlds = map(worldsFromHeightsFn, [80, 60, 40]);

    var utterances = ["John is tall", "null"];

    var params = { 
        parserWeights: createParserWeights(semppl.grammarIsTall),
        networkParams: { tall: { W: [W0, W1], b: [b0, b1] } },
    };

    var thetaCandidates = map(function(theta) {
        return {
            tall: theta
        };
    }, [8, 6, 4, 80, 60, 40]);

    //display(params.parserWeights);
    // Measure network output for John
    // Try out with several dimensions in each world. 
    var distOnPeopleWorlds = listener("John is tall", params, thetaCandidates, peopleWorlds, utterances);
    factor(distOnPeopleWorlds.score(peopleWorlds[0]));

    var distOnBuildingsWorlds = listener("John is tall", params, thetaCandidates, buildingsWorlds, utterances);
    factor(distOnBuildingsWorlds.score(buildingsWorlds[0]));

    return {
        people: distOnPeopleWorlds,
        buildings: distOnBuildingsWorlds
    };
};
var runTest = function(opts) {
    display(sample(OptimizeThenSample(model, {steps: 2000, estimator: {ELBO: {samples: 1}}, verbose: opts.verbose})));
}

runTest({ verbose: true });

/* Experiments:
 * Literal listener with theta
 * Pragmatic listener with theta
 * Same experiments with two contexts
 * 1. Top priority - Pragmatic, varying theta, 2 contexts
 */
