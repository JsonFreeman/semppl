// webppl rsa.wppl --require .
var nnparam = function(a, b) {
  return tensorParam([a, b], 0, 0.1);
};

var cartesianProductMap = function(fn, arr1, arr2) {
    return _.flatten(map(function(e1) {
        return map(function(e2) {
            return fn(e1, e2);
        }, arr2);
    }, arr1));
}

var featureFn = function(d) { return semppl.ruleFeatureFn(d); };
var scoreFn = function(features, params) {
    return sum(_.values(mapObject(function(k, v) {
        return _.has(params, k) ? v * params[k] : 0;
    }, features)));
};

var normalize = function(array) {
    var s = sum(array);
    return map(function(x) {
        return x / s;
    }, array);
};

var denotationProbability = function(world) {
    return function(derivation) {
        return derivation.semantics(world);
    };
};

var parser = semppl.createParser(semppl.grammarIsTall, semppl.ruleFeatureFn);

var parse = function(utterance, params, startSymbol) {
    // Need to use .call to call parser as a js function
    var chart = parser.call(null, utterance, params);
    var rootCellDerivations = semppl.getRootCellDerivations(chart, startSymbol);

    // Gather the two relevant probabilities
    var derivationProbabilities = normalize(map(function(d) {
        return Math.exp(scoreFn(featureFn(d), params.parserWeights));
    }, rootCellDerivations));

    return function(world) {
        // World dependent part starts here.
        var truthProbabilities = map(denotationProbability(world), rootCellDerivations);
        
        // We now have probabilities that the sentences are true given derivations P(t | d)
        // Also probabilities that derivations are correct P(d)

        // Marginalize out derivation by summing over derivations, multiply the two probabilities
        return sum(map2(function(derivation, truth) {
            return derivation * truth;
        }, derivationProbabilities, truthProbabilities));
    };
};

var sampleMatrixGaussian = function(dims,mean,variance,guide_mean){
  var length = dims[0]*dims[1]
  var g = sample(DiagCovGaussian({ mu: Vector(repeat(length, constF(mean))), sigma: Vector(repeat(length, constF(variance)))}),
    {guide: DiagCovGaussian({ mu: T.reshape(guide_mean,[length,1]), sigma: Vector(repeat(length, constF(0.001)))})})
  return T.reshape(g,dims)
}

var sampleScalarGaussian = function(mean, variance) {
    return sample(Gaussian({ mu: mean, sigma: variance }),
        { guide: Gaussian({ mu: scalarParam(mean), sigma: 0.001 }) });
}

var factorConjunction = function(dist, support) {
    var probabilityOfConjunction = sum(map(function(e) {
        return dist.score(e);
    }, support));
    factor(probabilityOfConjunction);
}

var createParserWeights = function(grammar) {
    var arrow = ' -> ';
    return _.object(map(function(entry) {
        return [
            entry.LHS + arrow + entry.RHS, // key
            sampleScalarGaussian(0, 1) // value
        ];
    }, grammar));
};

var sampleUniformPrior = function(support) {
    return support[randomInteger(support.length)];
};

var addTheta = function(params, theta) {
    return _.assign(_.clone(params), { theta: theta });
}

var literalListenerFn = function(params) {
    return cache(function(utterance, theta, worlds) {
        Infer({method: 'enumerate'}, function() {
            var truthFn = parse(utterance, addTheta(params, theta), "$S");
            var world = sampleUniformPrior(worlds);
            factor(Math.log(truthFn(world)));
            return world;
        })
    })
}

var speakerFn = function(params) {
    var literalListener = literalListenerFn(params);
    return cache(function(world, theta, worlds, utterances) {
        Infer({method: 'enumerate'}, function() {
            var utterance = sampleUniformPrior(utterances);
            var L = literalListener(utterance, theta, worlds);
            factor(L.score(world));
            return utterance;
        })
    })
}

var listenerFn = function(params) {
    var speaker = speakerFn(params);
    return cache(function(utterance, thetaCandidates, worlds, utterances) {
        Infer({method: 'enumerate'}, function() {
            var theta = sampleUniformPrior(thetaCandidates);
            var world = sampleUniformPrior(worlds);
            var S = speaker(world, theta, worlds, utterances);
            factor(S.score(utterance));
            return world;
        })
    })
}

var createNetworkParams = function(hiddenSize, inputSize) {
    var W0_var = nnparam(hiddenSize,inputSize)
    var W1_var = nnparam(1,hiddenSize)
    var b0_var = nnparam(hiddenSize,1)
    var b1_var = nnparam(1,1)

    var W0 = sampleMatrixGaussian([hiddenSize,inputSize],0,10,W0_var)
    var W1 = sampleMatrixGaussian([1,hiddenSize],0,10,W1_var)
    var b0 = sampleMatrixGaussian([hiddenSize,1],0,10,b0_var)
    var b1 = sampleMatrixGaussian([1,1],0,10,b1_var)

    return { W: [W0, W1], b: [b0, b1] };
}

var model = function() {
    var worldsFromHeightsFn = function(h, w) {
        return {
            facts: { height: { john: h }, weight: { john: w } },
            domain: ["john"]
        };
    };
    var peopleWorlds = cartesianProductMap(worldsFromHeightsFn, [8, 6, 4], [8, 6, 4]);
    var buildingsWorlds = cartesianProductMap(worldsFromHeightsFn, [80, 60, 40], [80, 60, 40]);

    var utterances = ["John is tall", "John is heavy", "John is short", "John is light", "null"];

    var scalarAdjectives = ['tall', 'heavy'];

    var params = { 
        parserWeights: createParserWeights(semppl.grammarIsTall),
        networkParams: _.object(scalarAdjectives, repeat(scalarAdjectives.length, function() {
            createNetworkParams(10, 3)
        }))
    };

    var thetaCandidates = map(function(theta) {
        return {
            tall: theta,
            heavy: theta
        };
    }, [8, 6, 4, 80, 60, 40]);

    //display(params.parserWeights);
    var listener = listenerFn(params);
    var distOnPeopleWorlds = {
        tall: listener("John is tall", thetaCandidates, peopleWorlds, utterances),
        heavy: listener("John is heavy", thetaCandidates, peopleWorlds, utterances)
    };

    factorConjunction(distOnPeopleWorlds.tall, peopleWorlds.slice(0, 3));
    factorConjunction(distOnPeopleWorlds.heavy, [peopleWorlds[0], peopleWorlds[3], peopleWorlds[6]]);

    var distOnBuildingsWorlds = {
        tall: listener("John is tall", thetaCandidates, buildingsWorlds, utterances),
        heavy: listener("John is heavy", thetaCandidates, buildingsWorlds, utterances)
    };

    factorConjunction(distOnBuildingsWorlds.tall, buildingsWorlds.slice(0, 3));
    factorConjunction(distOnBuildingsWorlds.heavy, [buildingsWorlds[0], buildingsWorlds[3], buildingsWorlds[6]]);

    return {
        people: distOnPeopleWorlds,
        buildings: distOnBuildingsWorlds
    };
};
var runTest = function(opts) {
    display(sample(OptimizeThenSample(model, {steps: 2000, estimator: {ELBO: {samples: 1}}, verbose: opts.verbose})));
}

runTest({ verbose: true });

/**
 * mapData
 * cache issue - close over params
 * Tall versus heavy (or even big)
 * Try to do some compositional learning
 * Sample the class explicitly
 * "is a tall man" + "is a tall building"
 * 
 * Tasks: 
 *  - Generate data using ground truth
 *  +!- Try to do caching refactor
 *  +!- Introduce tall, heavy, big with a neural net for each
 *  - Refactor features and weights to add distinguisher
 *  - Implement intersective adjectives
 *  - Compositional learning:
 *    - Intersecting predicates: (multiply, first, second)
 *    - Different ways of applying predicate to entity: (apply to input, ignore input)
 *    - Introduce "John is not a tall man" - not(tall * man), not(tall) * man, not(man) * tall, not(man)
 *    - Try ambiguities with conjunction + disjunction
 */